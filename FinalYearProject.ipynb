{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udUgdXBWMxvz"
      },
      "source": [
        "# Importing all the libraries needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qUg5URmAQHS-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import glob\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm.notebook import tqdm\n",
        "from skimage.color import rgb2lab, lab2rgb\n",
        "\n",
        "\n",
        "\n",
        "# Select the device to be cuda in order to use the GPU\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFtwb07ygy8t"
      },
      "source": [
        "# Get the data in order to create the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GFO9KY3V2SB",
        "outputId": "7327b839-a9bf-481a-9fe5-f7dad0320f76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastai==2.4\n",
            "  Downloading fastai-2.4-py3-none-any.whl (187 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▊                              | 10 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 20 kB 31.8 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 30 kB 27.1 MB/s eta 0:00:01\r\u001b[K     |███████                         | 40 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 51 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 61 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 71 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 81 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 92 kB 14.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 102 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 112 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 122 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 133 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 143 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 153 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 163 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 174 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 184 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 187 kB 14.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from fastai==2.4) (0.11.1+cu111)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from fastai==2.4) (1.3.5)\n",
            "Requirement already satisfied: spacy<4 in /usr/local/lib/python3.7/dist-packages (from fastai==2.4) (2.2.4)\n",
            "Requirement already satisfied: pillow>6.0.0 in /usr/local/lib/python3.7/dist-packages (from fastai==2.4) (7.1.2)\n",
            "Requirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.7/dist-packages (from fastai==2.4) (1.0.2)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (from fastai==2.4) (21.1.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from fastai==2.4) (1.4.1)\n",
            "Collecting fastcore<1.4,>=1.3.8\n",
            "  Downloading fastcore-1.3.29-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from fastai==2.4) (3.2.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from fastai==2.4) (3.13)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from fastai==2.4) (21.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from fastai==2.4) (1.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fastai==2.4) (2.23.0)\n",
            "Collecting torch<1.10,>=1.7.0\n",
            "  Downloading torch-1.9.1-cp37-cp37m-manylinux1_x86_64.whl (831.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 831.4 MB 7.0 kB/s \n",
            "\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.4) (1.0.6)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.4) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.4) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.4) (2.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.4) (0.9.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.4) (1.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.4) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.4) (57.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.4) (1.21.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.4) (4.63.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.4) (7.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.4) (3.0.6)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<4->fastai==2.4) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<4->fastai==2.4) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<4->fastai==2.4) (3.10.0.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fastai==2.4) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fastai==2.4) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fastai==2.4) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fastai==2.4) (3.0.4)\n",
            "Collecting torchvision>=0.8.2\n",
            "  Downloading torchvision-0.12.0-cp37-cp37m-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.0 MB 1.8 MB/s \n",
            "\u001b[?25h  Downloading torchvision-0.11.3-cp37-cp37m-manylinux1_x86_64.whl (23.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 23.2 MB 1.2 MB/s \n",
            "\u001b[?25h  Downloading torchvision-0.11.2-cp37-cp37m-manylinux1_x86_64.whl (23.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 23.3 MB 69.6 MB/s \n",
            "\u001b[?25h  Downloading torchvision-0.11.1-cp37-cp37m-manylinux1_x86_64.whl (23.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 23.3 MB 1.4 MB/s \n",
            "\u001b[?25h  Downloading torchvision-0.10.1-cp37-cp37m-manylinux1_x86_64.whl (22.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.1 MB 5.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai==2.4) (1.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai==2.4) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai==2.4) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai==2.4) (3.0.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->fastai==2.4) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->fastai==2.4) (2018.9)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->fastai==2.4) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->fastai==2.4) (1.1.0)\n",
            "Installing collected packages: torch, torchvision, fastcore, fastai\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.11.1+cu111\n",
            "    Uninstalling torchvision-0.11.1+cu111:\n",
            "      Successfully uninstalled torchvision-0.11.1+cu111\n",
            "  Attempting uninstall: fastai\n",
            "    Found existing installation: fastai 1.0.61\n",
            "    Uninstalling fastai-1.0.61:\n",
            "      Successfully uninstalled fastai-1.0.61\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.9.1 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.9.1 which is incompatible.\u001b[0m\n",
            "Successfully installed fastai-2.4 fastcore-1.3.29 torch-1.9.1 torchvision-0.10.1\n"
          ]
        }
      ],
      "source": [
        "#Fastai library to get images required for training\n",
        "!pip install fastai==2.4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this in order to get the images for the celebA dataset\n",
        "# but change paths to coco_image_paths\n",
        "path = \"/content/drive/MyDrive/celebAimages/img_align_celeba\"  \n",
        "paths = glob.glob(path + \"/*.jpg\") # Grabbing all the image file names"
      ],
      "metadata": {
        "id": "qAOHD0neU3gv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xutqYFFNjNGJ"
      },
      "outputs": [],
      "source": [
        "#Run this in order to het the coco images\n",
        "from fastai.data.external import untar_data, URLs\n",
        "# Use a sample of around 20.000 pictures of the coco dataset for object detection\n",
        "coco_images_path = untar_data(URLs.COCO_SAMPLE)\n",
        "coco_images_path = str(coco_images_path) + \"/train_sample\"\n",
        "#Use glob module to find all the jpg files (the images needed for training)\n",
        "coco_image_paths = glob.glob(coco_images_path + \"/*.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In most of my experiments the random.seed(25) was used\n",
        "# the experiments were replicated with other random seeds as well  \n",
        "np.random.seed(25)"
      ],
      "metadata": {
        "id": "fWDj1Qk9VzWt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uU1mk7Q3k6YZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bc0fd85-89c1-45e4-c229-64fe11c4053e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21000\n",
            "8000\n",
            "1000\n"
          ]
        }
      ],
      "source": [
        "# Here, the parameters of how many images have been modified in order to create multiple test situations\n",
        "# Choose 21000 image paths randomly for all of them \n",
        "coco_paths_subset = np.random.choice(coco_image_paths, 21000, replace=False)\n",
        "print(len(coco_paths_subset))\n",
        "# Random indexes to do the permutations\n",
        "random_idxs = np.random.permutation(21000)\n",
        "#Choosing the first 20000 indexes for the training set\n",
        "training_idxs = random_idxs[:20000]\n",
        "#Choosing the other 1000 indexes for the validation set\n",
        "validation_idxs = random_idxs[20000:] # choosing last 2000 as validation set\n",
        "# get the training paths\n",
        "training_paths = coco_image_paths[0:8000]\n",
        "# get the validation paths\n",
        "validation_paths = coco_image_paths[20000:21000]\n",
        "\n",
        "#print(training_paths)\n",
        "print(len(training_paths))\n",
        "#print(validation_paths)\n",
        "print(len(validation_paths))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDRpVYu0Domn"
      },
      "source": [
        "# Create the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3lbTr6sQloS6"
      },
      "outputs": [],
      "source": [
        "# Run this cell in order to create the datasets\n",
        "# This was modified during the training to add data augmentations by using \n",
        "# self.transforms = transforms.Compose([ transforms.Resize((SIZE, SIZE),  Image.BICUBIC), transforms.RandomHorizontalFlip(), \n",
        "# if the image was a training image\n",
        "\n",
        "SIZE = 256\n",
        "class ColorizationDataset(Dataset):\n",
        "    def __init__(self, paths):\n",
        "        self.transforms = transforms.Resize((SIZE, SIZE),  Image.BICUBIC)\n",
        "        self.size = SIZE\n",
        "        self.paths = paths\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Open the image\n",
        "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
        "        # Transform the image to the corresponding sizes\n",
        "        img = self.transforms(img)\n",
        "        # Convert it to an np array\n",
        "        img = np.array(img)\n",
        "        # Convert from RGB representation to LAB and convert it to tensor\n",
        "        img_lab = rgb2lab(img).astype(\"float32\")\n",
        "        img_lab = transforms.ToTensor()(img_lab)\n",
        "        #Normalize the values   \n",
        "        L = img_lab[[0], ...] / 50. - 1. # Between -1 and 1\n",
        "        ab = img_lab[[1, 2], ...] / 110. # Between -1 and 1\n",
        "        \n",
        "        return {'L': L, 'ab': ab}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKoBI3ZYELKC"
      },
      "source": [
        "# Create Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yPgAf_sUERv6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee034a7e-125d-4e18-b2db-5b17aea1b017"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
          ]
        }
      ],
      "source": [
        "training_dataset = ColorizationDataset(training_paths)\n",
        "validation_dataset = ColorizationDataset(validation_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4-5Bp4MzGDX3"
      },
      "outputs": [],
      "source": [
        "training_dataloader = DataLoader(training_dataset, batch_size=16, num_workers = 2)\n",
        "validation_dataloader = DataLoader(validation_dataset, batch_size=16, num_workers = 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ECCV GENERATOR"
      ],
      "metadata": {
        "id": "UKCxeC4Zed3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class BaseColor(nn.Module):\n",
        "\tdef __init__(self):\n",
        "\t\tsuper(BaseColor, self).__init__()\n",
        "\n",
        "\t\tself.l_cent = 50.\n",
        "\t\tself.l_norm = 100.\n",
        "\t\tself.ab_norm = 110.\n",
        "\n",
        "\tdef normalize_l(self, in_l):\n",
        "\t\treturn (in_l-self.l_cent)/self.l_norm\n",
        "\n",
        "\tdef unnormalize_l(self, in_l):\n",
        "\t\treturn in_l*self.l_norm + self.l_cent\n",
        "\n",
        "\tdef normalize_ab(self, in_ab):\n",
        "\t\treturn in_ab/self.ab_norm\n",
        "\n",
        "\tdef unnormalize_ab(self, in_ab):\n",
        "\t\treturn in_ab*self.ab_norm"
      ],
      "metadata": {
        "id": "EEbl-nuWei0o"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from IPython import embed\n",
        "\n",
        "\n",
        "class ECCVGenerator(BaseColor):\n",
        "    def __init__(self, norm_layer=nn.BatchNorm2d):\n",
        "        super(ECCVGenerator, self).__init__()\n",
        "\n",
        "        model1=[nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=True),]\n",
        "        model1+=[nn.ReLU(True),]\n",
        "        model1+=[nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1, bias=True),]\n",
        "        model1+=[nn.ReLU(True),]\n",
        "        model1+=[norm_layer(64),]\n",
        "\n",
        "        model2=[nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=True),]\n",
        "        model2+=[nn.ReLU(True),]\n",
        "        model2+=[nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1, bias=True),]\n",
        "        model2+=[nn.ReLU(True),]\n",
        "        model2+=[norm_layer(128),]\n",
        "\n",
        "        model3=[nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1, bias=True),]\n",
        "        model3+=[nn.ReLU(True),]\n",
        "        model3+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True),]\n",
        "        model3+=[nn.ReLU(True),]\n",
        "        model3+=[nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1, bias=True),]\n",
        "        model3+=[nn.ReLU(True),]\n",
        "        model3+=[norm_layer(256),]\n",
        "\n",
        "        model4=[nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n",
        "        model4+=[nn.ReLU(True),]\n",
        "        model4+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n",
        "        model4+=[nn.ReLU(True),]\n",
        "        model4+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n",
        "        model4+=[nn.ReLU(True),]\n",
        "        model4+=[norm_layer(512),]\n",
        "\n",
        "        model5=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n",
        "        model5+=[nn.ReLU(True),]\n",
        "        model5+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n",
        "        model5+=[nn.ReLU(True),]\n",
        "        model5+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n",
        "        model5+=[nn.ReLU(True),]\n",
        "        model5+=[norm_layer(512),]\n",
        "\n",
        "        model6=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n",
        "        model6+=[nn.ReLU(True),]\n",
        "        model6+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n",
        "        model6+=[nn.ReLU(True),]\n",
        "        model6+=[nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=True),]\n",
        "        model6+=[nn.ReLU(True),]\n",
        "        model6+=[norm_layer(512),]\n",
        "\n",
        "        model7=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n",
        "        model7+=[nn.ReLU(True),]\n",
        "        model7+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n",
        "        model7+=[nn.ReLU(True),]\n",
        "        model7+=[nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True),]\n",
        "        model7+=[nn.ReLU(True),]\n",
        "        model7+=[norm_layer(512),]\n",
        "\n",
        "        model8=[nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=True),]\n",
        "        model8+=[nn.ReLU(True),]\n",
        "        model8+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True),]\n",
        "        model8+=[nn.ReLU(True),]\n",
        "        model8+=[nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True),]\n",
        "        model8+=[nn.ReLU(True),]\n",
        "\n",
        "        model8+=[nn.Conv2d(256, 313, kernel_size=1, stride=1, padding=0, bias=True),]\n",
        "\n",
        "        self.model1 = nn.Sequential(*model1)\n",
        "        self.model2 = nn.Sequential(*model2)\n",
        "        self.model3 = nn.Sequential(*model3)\n",
        "        self.model4 = nn.Sequential(*model4)\n",
        "        self.model5 = nn.Sequential(*model5)\n",
        "        self.model6 = nn.Sequential(*model6)\n",
        "        self.model7 = nn.Sequential(*model7)\n",
        "        self.model8 = nn.Sequential(*model8)\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.model_out = nn.Conv2d(313, 2, kernel_size=1, padding=0, dilation=1, stride=1, bias=False)\n",
        "        self.upsample4 = nn.Upsample(scale_factor=4, mode='bilinear')\n",
        "\n",
        "    def forward(self, input_l):\n",
        "        conv1_2 = self.model1(self.normalize_l(input_l))\n",
        "        conv2_2 = self.model2(conv1_2)\n",
        "        conv3_3 = self.model3(conv2_2)\n",
        "        conv4_3 = self.model4(conv3_3)\n",
        "        conv5_3 = self.model5(conv4_3)\n",
        "        conv6_3 = self.model6(conv5_3)\n",
        "        conv7_3 = self.model7(conv6_3)\n",
        "        conv8_3 = self.model8(conv7_3)\n",
        "        out_reg = self.model_out(self.softmax(conv8_3))\n",
        "\n",
        "        return self.unnormalize_ab(self.upsample4(out_reg))\n"
      ],
      "metadata": {
        "id": "2tqqchubfK6B"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train ECCV Model"
      ],
      "metadata": {
        "id": "NSqUAP2Ti6Cp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import SummaryWriter\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# Create a SummaryWriter instance\n",
        "# SummaryWriter writes event files to log_dir\n",
        "log_dir = \"/content/drive/MyDrive/Graphs/ECCV\"\n",
        "writer = SummaryWriter(log_dir)\n",
        "\n",
        "num_epochs = 2\n",
        "#loss function\n",
        "model = ECCVGenerator()\n",
        "#PATH = \"/content/drive/MyDrive/weights/EGGV2weights.pth\"\n",
        "#model.load_state_dict(torch.load(PATH))\n",
        "criterion = torch.nn.MSELoss() \n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.001)  \n",
        "data1 = next(iter(validation_dataloader))\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "  i = 0\n",
        "  running_loss = 0.0\n",
        "  for data in tqdm(training_dataloader):\n",
        "    i += 1\n",
        "    optimizer.zero_grad()\n",
        "    # get the predicted ab from the image\n",
        "    ab_out_images = model.forward(data['L'])\n",
        "    #ab_out_images = ab_out_images.detach()\n",
        "    #get the original ab from the image\n",
        "    real_ab_images = data['ab']\n",
        "    loss = criterion(ab_out_images, real_ab_images)\n",
        "    #backward\n",
        "    loss.backward()\n",
        "    #optimize\n",
        "    optimizer.step()\n",
        "    writer.add_scalar(\"Loss/train\", loss.item(), i + (epoch * 500))\n",
        "    running_loss += loss.item()\n",
        "print(\"finished training\")\n",
        "FILE = \"ECCV_Weights.pth\"\n",
        "torch.save(model.state_dict(), FILE)"
      ],
      "metadata": {
        "id": "xk0oReQgg4E4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pix2PixModel"
      ],
      "metadata": {
        "id": "gmAqSBKYWyYK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "pxzguCQ026li"
      },
      "outputs": [],
      "source": [
        "#Code from Pix2Pix implementation, better explained in the report\n",
        "# Unet with skip connections\n",
        "class UnetBlock(nn.Module):\n",
        "    def __init__(self, nf, ni, submodule=None, input_c=None, dropout=False,\n",
        "                 innermost=False, outermost=False):\n",
        "        super().__init__()\n",
        "        self.outermost = outermost\n",
        "        if input_c is None: input_c = nf\n",
        "        downconv = nn.Conv2d(input_c, ni, kernel_size=4,\n",
        "                             stride=2, padding=1, bias=False)\n",
        "        downrelu = nn.LeakyReLU(0.2, True)\n",
        "        downnorm = nn.BatchNorm2d(ni)\n",
        "        uprelu = nn.ReLU(True)\n",
        "        upnorm = nn.BatchNorm2d(nf)\n",
        "        \n",
        "        if outermost:\n",
        "            upconv = nn.ConvTranspose2d(ni * 2, nf, kernel_size=4,\n",
        "                                        stride=2, padding=1)\n",
        "            down = [downconv]\n",
        "            up = [uprelu, upconv, nn.Tanh()]\n",
        "            model = down + [submodule] + up\n",
        "        elif innermost:\n",
        "            upconv = nn.ConvTranspose2d(ni, nf, kernel_size=4,\n",
        "                                        stride=2, padding=1, bias=False)\n",
        "            down = [downrelu, downconv]\n",
        "            up = [uprelu, upconv, upnorm]\n",
        "            model = down + up\n",
        "        else:\n",
        "            upconv = nn.ConvTranspose2d(ni * 2, nf, kernel_size=4,\n",
        "                                        stride=2, padding=1, bias=False)\n",
        "            down = [downrelu, downconv, downnorm]\n",
        "            up = [uprelu, upconv, upnorm]\n",
        "            if dropout: up += [nn.Dropout(0.5)]\n",
        "            model = down + [submodule] + up\n",
        "        self.model = nn.Sequential(*model)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        if self.outermost:\n",
        "            return self.model(x)\n",
        "        else:\n",
        "            return torch.cat([x, self.model(x)], 1)\n",
        "\n",
        "class Unet(nn.Module):\n",
        "    def __init__(self, input_c=1, output_c=2, n_down=8, num_filters=64):\n",
        "        super().__init__()\n",
        "        unet_block = UnetBlock(num_filters * 8, num_filters * 8, innermost=True)\n",
        "        for _ in range(n_down - 5):\n",
        "            unet_block = UnetBlock(num_filters * 8, num_filters * 8, submodule=unet_block, dropout=True)\n",
        "        out_filters = num_filters * 8\n",
        "        for _ in range(3):\n",
        "            unet_block = UnetBlock(out_filters // 2, out_filters, submodule=unet_block)\n",
        "            out_filters //= 2\n",
        "        self.model = UnetBlock(output_c, out_filters, input_c=input_c, submodule=unet_block, outermost=True)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "TuvtcU983LxA"
      },
      "outputs": [],
      "source": [
        "#PatchGan discriminator\n",
        "class PatchDiscriminator(nn.Module):\n",
        "    def __init__(self, input_c, num_filters=64, n_down=3):\n",
        "        super().__init__()\n",
        "        model = [self.get_layers(input_c, num_filters, norm=False)]\n",
        "        model += [self.get_layers(num_filters * 2 ** i, num_filters * 2 ** (i + 1), s=1 if i == (n_down-1) else 2) \n",
        "                          for i in range(n_down)]\n",
        "                                                  \n",
        "        model += [self.get_layers(num_filters * 2 ** n_down, 1, s=1, norm=False, act=False)] \n",
        "                                                                                           \n",
        "        self.model = nn.Sequential(*model)                                                   \n",
        "        \n",
        "    def get_layers(self, ni, nf, k=4, s=2, p=1, norm=True, act=True):\n",
        "        layers = [nn.Conv2d(ni, nf, k, s, p, bias=not norm)]         \n",
        "        if norm: layers += [nn.BatchNorm2d(nf)]\n",
        "        if act: layers += [nn.LeakyReLU(0.2, True)]\n",
        "        return nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qex82Bs83xbi"
      },
      "outputs": [],
      "source": [
        "# A method to upde the loss of the gan\n",
        "class GANLoss(nn.Module):\n",
        "    def __init__(self, gan_mode='vanilla', real_label=1.0, fake_label=0.0):\n",
        "        super().__init__()\n",
        "        self.register_buffer('real_label', torch.tensor(real_label))\n",
        "        self.register_buffer('fake_label', torch.tensor(fake_label))\n",
        "        if gan_mode == 'vanilla':\n",
        "            self.loss = nn.BCEWithLogitsLoss()\n",
        "        elif gan_mode == 'lsgan':\n",
        "            self.loss = nn.MSELoss()\n",
        "    \n",
        "    def get_labels(self, preds, target_is_real):\n",
        "        if target_is_real:\n",
        "            labels = self.real_label\n",
        "        else:\n",
        "            labels = self.fake_label\n",
        "        return labels.expand_as(preds)\n",
        "    \n",
        "    def __call__(self, preds, target_is_real):\n",
        "        labels = self.get_labels(preds, target_is_real)\n",
        "        loss = self.loss(preds, labels)\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "VUF2HK3D3_2F"
      },
      "outputs": [],
      "source": [
        "# A method to initialise the weights\n",
        "def init_weights(net, init='norm', gain=0.02):\n",
        "    \n",
        "    def init_func(m):\n",
        "        classname = m.__class__.__name__\n",
        "        if hasattr(m, 'weight') and 'Conv' in classname:\n",
        "            if init == 'norm':\n",
        "                nn.init.normal_(m.weight.data, mean=0.0, std=gain)\n",
        "            elif init == 'xavier':\n",
        "                nn.init.xavier_normal_(m.weight.data, gain=gain)\n",
        "            elif init == 'kaiming':\n",
        "                nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
        "            \n",
        "            if hasattr(m, 'bias') and m.bias is not None:\n",
        "                nn.init.constant_(m.bias.data, 0.0)\n",
        "        elif 'BatchNorm2d' in classname:\n",
        "            nn.init.normal_(m.weight.data, 1., gain)\n",
        "            nn.init.constant_(m.bias.data, 0.)\n",
        "            \n",
        "    net.apply(init_func)\n",
        "    print(f\"model initialized with {init} initialization\")\n",
        "    return net\n",
        "\n",
        "def init_model(model, device):\n",
        "    model = model.to(device)\n",
        "    model = init_weights(model)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0F-23NWK4O3b"
      },
      "outputs": [],
      "source": [
        "# The main model from the pix2pix paper\n",
        "class MainModel(nn.Module):\n",
        "    def __init__(self, net_G=None, lr_G=2e-4, lr_D=2e-4, \n",
        "                 beta1=0.5, beta2=0.999, lambda_L1=100.):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.lambda_L1 = lambda_L1\n",
        "        \n",
        "        if net_G is None:\n",
        "            self.net_G = init_model(Unet(input_c=1, output_c=2, n_down=8, num_filters=64), self.device)\n",
        "        else:\n",
        "            self.net_G = net_G.to(self.device)\n",
        "        self.net_D = init_model(PatchDiscriminator(input_c=3, n_down=3, num_filters=64), self.device)\n",
        "        self.GANcriterion = GANLoss(gan_mode='vanilla').to(self.device)\n",
        "        self.L1criterion = nn.L1Loss()\n",
        "        self.opt_G = optim.Adam(self.net_G.parameters(), lr=lr_G, betas=(beta1, beta2))\n",
        "        self.opt_D = optim.Adam(self.net_D.parameters(), lr=lr_D, betas=(beta1, beta2))\n",
        "    \n",
        "    def set_requires_grad(self, model, requires_grad=True):\n",
        "        for p in model.parameters():\n",
        "            p.requires_grad = requires_grad\n",
        "        \n",
        "    def setup_input(self, data):\n",
        "        self.L = data['L'].to(self.device)\n",
        "        self.ab = data['ab'].to(self.device)\n",
        "        \n",
        "    def forward(self):\n",
        "        self.fake_color = self.net_G(self.L)\n",
        "    \n",
        "    def backward_D(self):\n",
        "        fake_image = torch.cat([self.L, self.fake_color], dim=1)\n",
        "        fake_preds = self.net_D(fake_image.detach())\n",
        "        self.loss_D_fake = self.GANcriterion(fake_preds, False)\n",
        "        real_image = torch.cat([self.L, self.ab], dim=1)\n",
        "        real_preds = self.net_D(real_image)\n",
        "        self.loss_D_real = self.GANcriterion(real_preds, True)\n",
        "        # Multiply by 0.5 to slow down the rate at which D learns relative to G\n",
        "        self.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5\n",
        "        self.loss_D.backward()\n",
        "    \n",
        "    def backward_G(self):\n",
        "        fake_image = torch.cat([self.L, self.fake_color], dim=1)\n",
        "        fake_preds = self.net_D(fake_image)\n",
        "        self.loss_G_GAN = self.GANcriterion(fake_preds, True)\n",
        "        self.loss_G_L1 = self.L1criterion(self.fake_color, self.ab) * self.lambda_L1\n",
        "        self.loss_G = self.loss_G_GAN + self.loss_G_L1\n",
        "        self.loss_G.backward()\n",
        "    \n",
        "    def optimize(self):\n",
        "        # Alternate between one gradient descent step on D\n",
        "        self.forward()\n",
        "        self.net_D.train()\n",
        "        self.set_requires_grad(self.net_D, True)\n",
        "        self.opt_D.zero_grad()\n",
        "        self.backward_D()\n",
        "        self.opt_D.step()\n",
        "        \n",
        "        # And then one sten on G\n",
        "        self.net_G.train()\n",
        "        self.set_requires_grad(self.net_D, False)\n",
        "        self.opt_G.zero_grad()\n",
        "        self.backward_G()\n",
        "        self.opt_G.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning"
      ],
      "metadata": {
        "id": "DvBmpr7saNwT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "2wKWDKVCTJvS"
      },
      "outputs": [],
      "source": [
        "# I have used the same approach in order to transfer the weights from resnet 34 as well\n",
        "# This could be done just by replacing resnet 18 with resnet 34 in the code below\n",
        "from fastai.vision.learner import create_body\n",
        "from torchvision.models.resnet import resnet18\n",
        "from fastai.vision.models.unet import DynamicUnet\n",
        "\n",
        "\n",
        "def build_res_unet(n_input=1, n_output=2, size=256):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    body = create_body(resnet18, pretrained=True, n_in=n_input, cut=-2)\n",
        "    net_G = DynamicUnet(body, n_output, (size, size)).to(device)\n",
        "    return net_G"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the Pix2Pix Model"
      ],
      "metadata": {
        "id": "vXsJ77TyeAo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code comenteed below is used in order to use transfer learning\n",
        "#net_G_loaded = build_res_unet(n_input=1, n_output=2, size=256)\n",
        "#net_G_loaded.load_state_dict(torch.load(\"/content/drive/MyDrive/FinalWeights/Resnet-18-pretrainedbyme8000pictures.pth\", map_location=device))\n",
        "#model = MainModel(net_G=net_G)\n",
        "\n",
        "model = MainModel()\n",
        "# Load TensorBoard notebook extension\n",
        "#%load_ext tensorboard\n",
        "\n",
        "# Import SummaryWriter\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# Create a SummaryWriter instance\n",
        "# SummaryWriter writes event files to log_dir\n",
        "log_dir = \"/content/drive/MyDrive/Graphs/Modelgraph\"\n",
        "writer = SummaryWriter(log_dir)\n",
        "epochs = 100\n",
        "for e in range(epochs):\n",
        "  i = 0\n",
        "  for data in tqdm(training_dataloader):\n",
        "    model.setup_input(data) \n",
        "    model.optimize()\n",
        "    i += 1\n",
        "    writer.add_scalar('Loss/loss_D_train', model.loss_D, i +  (e * 1000))\n",
        "    writer.add_scalar('Loss/loss_D_fake_train', model.loss_D_fake, i + (e * 1000))\n",
        "    writer.add_scalar('Loss/loss_D_real_train', model.loss_D_real, i + (e * 1000))\n",
        "    writer.add_scalar('Loss/loss_G', model.loss_G, i + (e * 1000))\n",
        "    writer.add_scalar('Loss/loss_G_GAN', model.loss_G_GAN, i + (e * 1000))\n",
        "    writer.add_scalar('Loss/loss_G_L1', model.loss_G_L1, i + (e * 1000))    \n",
        "\n",
        "  print(f\"\\nEpoch {e+1}/{epochs}\")\n",
        "  model_save_name = 'ModelName.pth'\n",
        "  path = f\"/content/drive/MyDrive/FinalWeights/{model_save_name}\" \n",
        "  torch.save(model.state_dict(), path)"
      ],
      "metadata": {
        "id": "E9QOCvrpx5b4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pretrain the generator"
      ],
      "metadata": {
        "id": "7elp_hmxdo8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import SummaryWriter\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# Create a SummaryWriter instance\n",
        "# SummaryWriter writes event files to log_dir\n",
        "log_dir = \"/content/drive/MyDrive/Graphs/NameOfTheGraph\"\n",
        "writer = SummaryWriter(log_dir)\n",
        "\n",
        "#Build the net\n",
        "net_G = build_res_unet(n_input=1, n_output=2, size=256)\n",
        "# Choose the optimizer\n",
        "opt = optim.Adam(net_G.parameters(), lr=1e-4)\n",
        "# Choose the loss\n",
        "criterion = nn.L1Loss() \n",
        "# Choose the number of epochs\n",
        "\n",
        "epochs = 100\n",
        "for e in range(epochs):\n",
        "  i = 0\n",
        "  for data in tqdm(training_dataloader):\n",
        "    i = i + 1\n",
        "    L, ab = data['L'].to(device), data['ab'].to(device)\n",
        "    preds = net_G(L)\n",
        "    loss = criterion(preds, ab)\n",
        "    opt.zero_grad()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    writer.add_scalar('Loss/loss_G', loss.item(), i +  (e * 500))\n",
        "  print(f\"Epoch {e + 1}/{epochs}\")\n",
        "  model_save_name = 'Model_save_name.pth'\n",
        "  path = f\"/content/drive/MyDrive/FinalWeights/{model_save_name}\"\n",
        "  torch.save(net_G.state_dict(), path)\n"
      ],
      "metadata": {
        "id": "GbtRPtqKQW0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gvkjgFXBtHbm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "name": "FinalYearProject.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}